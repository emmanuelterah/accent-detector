# üéôÔ∏è Accent Analysis Tool

A powerful web application that analyzes accents in videos using advanced machine learning models. This tool can detect various English accents (British, American, Australian, Indian, and Other) from video content, providing both transcription and accent classification with confidence scores.

## ‚ú® Features

- **Video Processing**
  - Supports YouTube and Google Drive video URLs
  - Automatic video downloading and audio extraction
  - Smart format selection for optimal quality

- **Accent Analysis**
  - Detects 5 different English accents:
    - British
    - American
    - Australian
    - Indian
    - Other
  - Provides confidence scores for predictions
  - Includes full speech transcription

- **Modern UI**
  - Beautiful glassmorphism design
  - Responsive layout
  - Real-time processing feedback
  - Clean and intuitive interface

## üõ†Ô∏è Technical Stack

- **Backend**
  - Python 3.x
  - Streamlit for web interface
  - PyTorch for deep learning
  - Transformers library for accent classification
  - Faster-Whisper for speech recognition
  - yt-dlp for video downloading
  - FFmpeg for audio processing

- **Models**
  - Whisper (base) for speech transcription
  - Custom accent classification model (dima806/english_accents_classification)

## üìã Prerequisites

- Python 3.8 or higher
- FFmpeg installed on your system
- CUDA-capable GPU (optional, but recommended for faster processing)

## üöÄ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/emmanuelterah/accent-detector.git
   cd accent-detector
   ```

2. Create and activate a virtual environment (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Install FFmpeg:
   - **Windows**: Download from [FFmpeg website](https://ffmpeg.org/download.html) and add to PATH
   - **Linux**: `sudo apt-get install ffmpeg`
   - **macOS**: `brew install ffmpeg`

## üéÆ Usage

1. Start the application:
   ```bash
   streamlit run app.py
   ```

2. Open your web browser and navigate to the provided URL (typically http://localhost:8501)

3. Enter a video URL (YouTube or Google Drive) in the input field

4. Click "Analyze Accent" and wait for the results

5. View the results:
   - Detected accent
   - Confidence score
   - Full transcription
   - Detailed explanation

## üåê Public Access

The application is publicly accessible at: [http://104.131.51.248:8501/](http://104.131.51.248:8501/)

## üîß Configuration

The application uses several configuration parameters that can be modified in the code:

- `MODEL_CACHE_DIR`: Directory for caching ML models
- `TEMP_DIR`: Directory for temporary files
- Audio processing duration (default: 60 seconds)
- Model parameters and settings

## üìù Notes

- The application processes the first 60 seconds of audio for analysis
- Temporary files are automatically cleaned up after processing
- For best results, use videos with clear speech and minimal background noise
- Processing time may vary depending on video length and system specifications

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgments

- [Faster-Whisper](https://github.com/guillaumekln/faster-whisper) for speech recognition
- [yt-dlp](https://github.com/yt-dlp/yt-dlp) for video downloading
- [Streamlit](https://streamlit.io/) for the web interface
- [Hugging Face](https://huggingface.co/) for the accent classification model

## üéè Deployment Options

### 1. Cloud Deployment (Recommended)

#### A. DigitalOcean (Current Deployment)
- **Current Droplet Specifications**:
  - RAM: 4GB
  - vCPUs: 2
  - Storage: 80GB SSD
  - Region: NYC3
- **Why DigitalOcean**:
  - Cost-effective for ML applications
  - Simple and straightforward deployment
  - Excellent performance for Python applications
  - Built-in monitoring and analytics
  - Easy scaling options
  - Reliable uptime and performance
  - Great documentation and community support
- **Current Performance**:
  - Handles multiple concurrent users
  - Stable processing of video analysis
  - Efficient resource utilization
  - Quick response times

#### B. Google Cloud Platform (GCP)
- **Recommended Instance**: 
  - Compute Engine: n1-standard-4 or better
  - GPU: NVIDIA T4 or better
  - Storage: 50GB+ SSD
- **Benefits**:
  - High-performance GPU support
  - Scalable resources
  - Global CDN
  - Cost-effective for production

#### C. AWS
- **Recommended Instance**:
  - EC2: g4dn.xlarge or better
  - GPU: NVIDIA T4
  - Storage: EBS 50GB+
- **Benefits**:
  - Enterprise-grade reliability
  - Advanced monitoring
  - Easy scaling

#### C. Azure
- **Recommended Instance**:
  - NC4as_T4_v3 or better
  - GPU: NVIDIA T4
  - Storage: 50GB+ Premium SSD
- **Benefits**:
  - Microsoft ecosystem integration
  - Strong security features
  - Global presence

### 2. Local Deployment

For development or testing:
- **Minimum Requirements**:
  - CPU: 8 cores
  - RAM: 16GB
  - GPU: NVIDIA GTX 1660 or better
  - Storage: 50GB SSD
- **Benefits**:
  - Full control over environment
  - No cloud costs
  - Faster development cycle

## üí™ Maximizing Capabilities

### 1. Performance Optimization

- **GPU Acceleration**:
  ```python
  # In accent_analyzer.py
  self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  ```
  - Enable CUDA for faster processing
  - Use batch processing for multiple videos
  - Implement model quantization for faster inference

- **Memory Management**:
  ```python
  # Automatic cleanup
  torch.cuda.empty_cache()
  gc.collect()
  ```
  - Regular memory cleanup
  - Efficient model loading
  - Temporary file management

### 2. Scaling Features

- **Batch Processing**:
  - Process multiple videos simultaneously
  - Queue system for large workloads
  - Progress tracking

- **API Integration**:
  - RESTful API endpoints
  - WebSocket for real-time updates
  - Rate limiting and authentication

### 3. Advanced Usage

- **Custom Model Training**:
  - Fine-tune accent classification
  - Add new accent types
  - Improve accuracy with custom datasets

- **Extended Analysis**:
  - Detailed accent characteristics
  - Regional variations
  - Confidence breakdowns

## üîß Production Setup

1. **Environment Variables**:
   ```bash
   export CUDA_VISIBLE_DEVICES=0
   export MODEL_CACHE_DIR=/path/to/cache
   export TEMP_DIR=/path/to/temp
   ```

2. **System Configuration**:
   ```bash
   # GPU Memory Management
   nvidia-smi -c 3
   # FFmpeg Optimization
   export FFMPEG_THREADS=4
   ```

3. **Monitoring Setup**:
   - Prometheus metrics
   - Grafana dashboards
   - Error tracking

## üìà Performance Metrics

- **Processing Speed**:
  - Video download: 2-5 seconds
  - Audio extraction: 1-2 seconds
  - Transcription: 5-10 seconds
  - Accent analysis: 2-3 seconds

- **Resource Usage**:
  - GPU Memory: 2-4GB
  - CPU Usage: 30-50%
  - Storage: 100MB per video

## üîê Security Considerations

- **Input Validation**:
  - URL sanitization
  - File type verification
  - Size limits

- **Resource Protection**:
  - Rate limiting
  - Request queuing
  - Memory caps 